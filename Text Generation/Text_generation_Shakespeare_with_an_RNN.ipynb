{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q-1uyQwf0fo"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dBiem3cEf0Qy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P6mm4jGgaa8"
      },
      "source": [
        "#### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3ZT03jffcVg",
        "outputId": "5ace376d-6e71-4ed0-bc90-8fc670331922",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_downloaded_file = tf.keras.utils.get_file(\n",
        "    fname = 'shakespeare.txt',\n",
        "    origin = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
        "    cache_subdir = '/content/',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SabARrTSgc0c"
      },
      "source": [
        "#### Explore data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q63L7_lAgj38",
        "outputId": "907a79d8-b84a-4664-8f2a-ac27776ef692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus in 'shakespeare.txt' has 1,115,394 characters.\n",
            "\n",
            "Starts with: \n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You...\n"
          ]
        }
      ],
      "source": [
        "with open(path_to_downloaded_file, 'rb') as file:\n",
        "    text = file.read().decode(encoding='utf-8')\n",
        "\n",
        "print(f\"Corpus in '{os.path.basename(path_to_downloaded_file)}' has {len(text):,} characters.\\n\")\n",
        "print(f\"Starts with: \\n\\n{text[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3FgMRczhljF",
        "outputId": "a0bfd400-947b-47a6-bff6-2cfcdc0e3d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus has 65 unique characters:\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print(f\"Corpus has {VOCAB_SIZE:,} unique characters:\")\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsTEl85nLSf"
      },
      "source": [
        "#### **`StringLookup`**\n",
        "\n",
        "- maps a set of arbitrary input strings into (possibly encoded) integer indices output, via a table-based vocabulary lookup.\n",
        "- performs no splitting or transformation of input strings.\n",
        "- The vocabulary for the layer must be either supplied on construction or learned via `adapt()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMVke9z1lbuR",
        "outputId": "c184d89f-b5a3-4524-d48b-464eb580404e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(48,), dtype=int64, numpy=\n",
              "array([38, 54, 60,  2, 42, 40, 53,  2, 53, 44, 61, 44, 57,  2, 41, 44,  2,\n",
              "       54, 61, 44, 57, 43, 57, 44, 58, 58, 44, 43,  1,  1, 28, 57,  2,  0,\n",
              "        2, 54, 61, 44, 57, 44, 43, 60, 42, 40, 59, 44, 43,  9])>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chars_to_ids_layer = tf.keras.layers.StringLookup(vocabulary = vocab)\n",
        "\n",
        "# Test\n",
        "token_ids = chars_to_ids_layer(list(\"You can never be overdressed\\n\\nOr + overeducated.\"))\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYGrNNq_mDSK",
        "outputId": "a18f068b-6652-4525-bc2c-dc03fe21638d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(48,), dtype=string, numpy=\n",
              "array([b'Y', b'o', b'u', b' ', b'c', b'a', b'n', b' ', b'n', b'e', b'v',\n",
              "       b'e', b'r', b' ', b'b', b'e', b' ', b'o', b'v', b'e', b'r', b'd',\n",
              "       b'r', b'e', b's', b's', b'e', b'd', b'\\n', b'\\n', b'O', b'r', b' ',\n",
              "       b'[UNK]', b' ', b'o', b'v', b'e', b'r', b'e', b'd', b'u', b'c',\n",
              "       b'a', b't', b'e', b'd', b'.'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "ids_to_chars_layer = tf.keras.layers.StringLookup(\n",
        "                    vocabulary = chars_to_ids_layer.get_vocabulary(),\n",
        "                    invert=True,\n",
        "                )\n",
        "\n",
        "# Test\n",
        "tokens = ids_to_chars_layer(token_ids)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kXUg38smvRR",
        "outputId": "16a969b1-07df-4c10-d080-1e46d63d4065"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'You can never be overdressed\\n\\nOr [UNK] overeducated.'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def ids_to_text(input):\n",
        "    output = ids_to_chars_layer(input)\n",
        "    output = tf.strings.reduce_join(output, axis=-1).numpy()\n",
        "    return output\n",
        "\n",
        "ids_to_text(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUelUdoqoccv"
      },
      "source": [
        "## The Prediction Task\n",
        "\n",
        "The goal is to, given a character, or a sequence of characters, to predict what is the most probable next character?\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen element/s, given all the characters computed until this moment, what is the next character?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCbByrdbpesP"
      },
      "source": [
        "### Encode data into token ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE4Ldf1qpgDZ",
        "outputId": "05adcc20-faed-4342-edbe-54d705a595da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19 -> 'F', 48 -> 'i', 57 -> 'r', 58 -> 's', 59 -> 't', 2 -> ' ', 16 -> 'C', 48 -> 'i', 59 -> 't', 48 -> 'i', "
          ]
        }
      ],
      "source": [
        "# split corpus into chars\n",
        "text_chars = tf.strings.unicode_split(text, 'UTF-8')\n",
        "# encode chars into int indices based on vocab\n",
        "text_ids = chars_to_ids_layer(text_chars)\n",
        "# create dataset from tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(text_ids)\n",
        "\n",
        "for id in dataset.take(10):\n",
        "    print(f\"{id.numpy()} -> '{ids_to_chars_layer(id).numpy().decode('utf-8')}'\", end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe9SHMOVxoAk"
      },
      "source": [
        "### Batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3wfC3yXsnp_",
        "outputId": "01669190-96f9-47ab-da70-06bcebc2e0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch ids:\n",
            "[19 48 57 58 59  2 16 48 59 48 65 44 53 11  1 15 44 45 54 57 44  2 62 44\n",
            "  2 55 57 54 42 44 44 43  2 40 53 64  2 45 60 57 59 47 44 57  7  2 47 44\n",
            " 40 57  2 52 44  2 58 55 44 40 50  9  1  1 14 51 51 11  1 32 55 44 40 50\n",
            "  7  2 58 55 44 40 50  9  1  1 19 48 57 58 59  2 16 48 59 48 65 44 53 11\n",
            "  1 38 54 60  2]\n",
            "\n",
            "Batch chars:\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' ']\n",
            "\n",
            "Batch chars joined:\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "SEQ_LENGTH = 100\n",
        "\n",
        "sequenced_ds = dataset.batch(SEQ_LENGTH+1, drop_remainder=True)\n",
        "\n",
        "for batch in sequenced_ds.take(1):\n",
        "    print(f\"Batch ids:\\n{batch}\", end='\\n\\n')\n",
        "    print(f\"Batch chars:\\n{ids_to_chars_layer(batch)}\", end='\\n\\n')\n",
        "    print(f\"Batch chars joined:\\n{ids_to_text(batch)}\", end='\\n\\n')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhvg6jBBtpGQ"
      },
      "source": [
        "### Label data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2uM4FRptsla",
        "outputId": "38e68f3f-3382-4c07-beef-0c9803ad5595"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Hello World', 'ello World!')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def split_sequence(seq):\n",
        "    input_seq = seq[:-1]\n",
        "    target_seq = seq[1:]\n",
        "    return (input_seq, target_seq)\n",
        "\n",
        "split_sequence(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFOCDfsOubqt",
        "outputId": "cee7a4a7-8f22-426f-f068-83ea584d8a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
            "b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us k\"\n",
            "b\"ow Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be \"\n",
            "b\"l him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor cit'\n",
            "b'ne: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "final_ds = sequenced_ds.map(lambda seq: split_sequence(seq))\n",
        "\n",
        "for input_batch, target_batch in final_ds.take(5):\n",
        "    print(ids_to_text(input_batch))\n",
        "    print(ids_to_text(target_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively"
      ],
      "metadata": {
        "id": "JqUWAoY3iLwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = dataset.window(SEQ_LENGTH+1, shift=1, drop_remainder=True)\n",
        "ds = ds.flat_map(lambda window: window.batch(SEQ_LENGTH+1))\n",
        "ds = ds.shuffle(1000).batch(64, drop_remainder=True)\n",
        "ds = ds.map(lambda batch: (batch[:, :-1], batch[:, 1:]))\n",
        "\n",
        "next(iter(ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2IXZi4NiMwG",
        "outputId": "8a2acb69-ce09-4778-b597-831b5a4769e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(64, 100), dtype=int64, numpy=\n",
              " array([[51, 44, 40, ..., 60, 53, 43],\n",
              "        [53, 11,  1, ..., 36, 47, 40],\n",
              "        [57, 11,  2, ..., 47, 44, 48],\n",
              "        ...,\n",
              "        [62,  2, 16, ..., 50, 48, 51],\n",
              "        [48, 65, 44, ..., 60, 59,  2],\n",
              "        [44, 58, 54, ..., 62,  6, 59]])>,\n",
              " <tf.Tensor: shape=(64, 100), dtype=int64, numpy=\n",
              " array([[44, 40, 53, ..., 53, 43, 40],\n",
              "        [11,  1, 28, ..., 47, 40, 59],\n",
              "        [11,  2, 59, ..., 44, 48, 57],\n",
              "        ...,\n",
              "        [ 2, 16, 40, ..., 48, 51, 51],\n",
              "        [65, 44, 53, ..., 59,  2, 59],\n",
              "        [58, 54, 51, ...,  6, 59,  7]])>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DumJPCcp1Z8z"
      },
      "source": [
        "### Create training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6jrqNja2jFS",
        "outputId": "91f989a3-3576-4266-9b75-8323b3c62ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input (64, 100) [0]: b'she took to quench it,\\nShe would to each one sip. You are retired,\\nAs if you were a feasted one and '\n",
            "Target (64, 100) [0]: b'he took to quench it,\\nShe would to each one sip. You are retired,\\nAs if you were a feasted one and n'\n"
          ]
        }
      ],
      "source": [
        "training_ds = final_ds.shuffle(10000).batch(64, drop_remainder=True)\n",
        "\n",
        "for input_batch, target_batch in training_ds.take(1):\n",
        "    print(f\"Input {input_batch.shape} [0]: {ids_to_text(input_batch[0])}\")\n",
        "    print(f\"Target {target_batch.shape} [0]: {ids_to_text(target_batch[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpUBYLa-3hRo"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "x7YsNtgM6UzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb6dd94-47c1-4861-9334-fd9e69945737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 66\n"
          ]
        }
      ],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(chars_to_ids_layer.get_vocabulary())\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size),\n",
        "], name=\"my_model_sequential\")"
      ],
      "metadata": {
        "id": "FPkmGK73pZUm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9u5aPc_hRw"
      },
      "source": [
        "To get actual predictions from the model we sample from the output distribution, to get the character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNQR3bgL7RTM",
        "outputId": "064e382a-abf5-4e59-fe0d-7c894dc5c4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch (64, 100) # (batch_size, sequence_length)\n",
            "\n",
            "Prediction batch (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "\n",
            "1st prediction (100, 66):\n",
            "[[ -3.8910062    0.7578387    4.126569   ...  -2.956409     1.9454858\n",
            "   -3.6718848 ]\n",
            " [ -4.7899528    2.3632624    5.705147   ...  -3.7940722   -0.13120429\n",
            "   -5.547408  ]\n",
            " [ -4.0349226    1.1451188    2.9610977  ...  -0.49251893  -0.41288298\n",
            "   -1.7498076 ]\n",
            " ...\n",
            " [ -8.190473    17.590609    14.522903   ...  -8.360919     0.33438018\n",
            "  -12.467685  ]\n",
            " [ -4.474937     1.2537353   -7.873265   ...  -4.800757     1.0759474\n",
            "   -7.827163  ]\n",
            " [ -5.454521    -4.722344    -1.6846175  ...  -5.206412     5.873618\n",
            "   -6.058078  ]]\n",
            "\n",
            "Next char predictions:\n",
            "\n",
            "slp aur geik,  ahe  fempecteIot;is;\n",
            "\n",
            "FIdsenger:\n",
            "Sor  Bt iou ll ppye tour mofe, fai io tour donse.\n",
            "Ch\n"
          ]
        }
      ],
      "source": [
        "for input_batch, target_batch in training_ds.take(1):\n",
        "    print(f\"Input batch {input_batch.shape} # (batch_size, sequence_length)\\n\")\n",
        "    prediction_batch = model(input_batch)\n",
        "    print(f\"Prediction batch {prediction_batch.shape} # (batch_size, sequence_length, vocab_size)\\n\")\n",
        "    print(f\"1st prediction {prediction_batch[0].shape}:\\n{prediction_batch[0]}\\n\")\n",
        "\n",
        "    # sample from output distribution, defined by the logits over the character vocabulary\n",
        "    sampled_seq = tf.random.categorical(prediction_batch[0], num_samples=1)\n",
        "    sampled_seq = tf.squeeze(sampled_seq)\n",
        "    print(f\"Next char predictions:\\n\\n{ids_to_text(sampled_seq).decode('utf-8')}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "er17nU8e_MfI",
        "outputId": "b634d822-b2a7-4ff3-94db-48cc3fe17e70"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"my_model_sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model_sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │          \u001b[38;5;34m16,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m)             │       \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)               │          \u001b[38;5;34m67,650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,594\u001b[0m (15.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,594</span> (15.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,594\u001b[0m (15.34 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,594</span> (15.34 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-WtLD3UCqBa"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Rf0Cxa2gEKAP"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = loss_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes.\n",
        "\n",
        "To confirm this we check that the exponential of the loss is approximately equal to the vocabulary size."
      ],
      "metadata": {
        "id": "7BLsRvYxB5Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(target_batch, prediction_batch)\n",
        "print(loss.numpy())\n",
        "tf.exp(loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO6BUEjzACQz",
        "outputId": "5e820a53-27fa-4f29-e9e3-b4b789071dd7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8395946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3154283"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use **`tf.keras.callbacks.ModelCheckpoint`** to ensure that checkpoints are saved during training."
      ],
      "metadata": {
        "id": "jJwZjwP7CMDb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "YKbPVKo9Eb_4"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_ds = training_ds.cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WWSQlU-mCkFG"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WXwAYP2EPAh",
        "outputId": "20499bd7-c7d0-44bf-d730-1a40c28283a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.8155\n",
            "Epoch 2/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7691\n",
            "Epoch 3/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - loss: 0.7595\n",
            "Epoch 4/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 62ms/step - loss: 0.7559\n",
            "Epoch 5/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 60ms/step - loss: 0.7552\n",
            "Epoch 6/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7508\n",
            "Epoch 7/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.7468\n",
            "Epoch 8/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.7414\n",
            "Epoch 9/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.7377\n",
            "Epoch 10/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7347\n",
            "Epoch 11/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7357\n",
            "Epoch 12/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7355\n",
            "Epoch 13/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7406\n",
            "Epoch 14/20\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.7433\n",
            "Epoch 15/20\n",
            "\u001b[1m 86/172\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - loss: 0.7400"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "            training_ds,\n",
        "            epochs = 20,\n",
        "            callbacks = [checkpoint_callback],\n",
        "        )\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in training_ds.take(1):\n",
        "    prediction_batch = model(input_batch)\n",
        "    print(f\"Prediction batch {prediction_batch.shape} # (batch_size, sequence_length, vocab_size)\\n\")\n",
        "    print(f\"1st prediction {prediction_batch[0].shape}:\\n{prediction_batch[0]}\\n\")\n",
        "\n",
        "    # sample from output distribution, defined by the logits over the character vocabulary\n",
        "    sampled_seq = tf.random.categorical(prediction_batch[0], num_samples=1)\n",
        "    sampled_seq = tf.squeeze(sampled_seq)\n",
        "    print(f\"Next char predictions:\\n{ids_to_text(sampled_seq).decode('utf-8')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nno6TAJoDPk8",
        "outputId": "9ebbb9b5-6d35-4f7d-c4d2-4291072f470f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction batch (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
            "\n",
            "1st prediction (100, 66):\n",
            "[[ -3.8910062    0.7578387    4.126569   ...  -2.956409     1.9454858\n",
            "   -3.6718848 ]\n",
            " [ -4.7899528    2.3632624    5.705147   ...  -3.7940722   -0.13120429\n",
            "   -5.547408  ]\n",
            " [ -4.0349226    1.1451188    2.9610977  ...  -0.49251893  -0.41288298\n",
            "   -1.7498076 ]\n",
            " ...\n",
            " [ -8.190473    17.590609    14.522903   ...  -8.360919     0.33438018\n",
            "  -12.467685  ]\n",
            " [ -4.474937     1.2537353   -7.873265   ...  -4.800757     1.0759474\n",
            "   -7.827163  ]\n",
            " [ -5.454521    -4.722344    -1.6846175  ...  -5.206412     5.873618\n",
            "   -6.058078  ]]\n",
            "\n",
            "Next char predictions:\n",
            "wsdeour newk\n",
            ",\n",
            "ahen auatect Iot os \n",
            "\n",
            "RIssenger:\n",
            "Sur, tf hou ll kaye your laee, ooy:to your ponse;\n",
            "Mh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text"
      ],
      "metadata": {
        "id": "ygblypk-DXce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We predict the next character in a sequence given a context and a temperature parameter controls the randomness of predictions."
      ],
      "metadata": {
        "id": "OPTfpqYswaPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_chars = tf.strings.unicode_split(['ROMEO:'], 'UTF-8').to_tensor()\n",
        "chars_to_ids_layer(input_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w6kfXrv2HjL",
        "outputId": "dd02fa37-6a68-403b-a247-7f70ada236af"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 6), dtype=int64, numpy=array([[31, 28, 26, 18, 28, 11]])>"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(input, temperature=1.0):\n",
        "    # Let's generate text using the trained model:\n",
        "    input_chars = tf.strings.unicode_split(input, 'UTF-8')\n",
        "    input_ids = chars_to_ids_layer(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits = model.predict(input_ids, verbose=0)\n",
        "    predicted_logits /= temperature\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits[0, :], num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)[-1]\n",
        "    return ids_to_chars_layer(predicted_ids).numpy().decode('utf-8')\n",
        "\n",
        "\n",
        "#tf.random.set_seed(42)\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "text = ''\n",
        "for _ in range(1000):\n",
        "    text += predict_next_char(next_char)\n",
        "    next_char = [text]\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTH_tTcjr0WQ",
        "outputId": "b43798d8-f905-4002-efb4-bde0b680f67a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HASTGRET:\n",
            "Poison do, or breather; thick may long now!\n",
            "Why, this is ready to be the vow; and knee\n",
            "executed against those ears?\n",
            "\n",
            "LEONTES:\n",
            "Stay, of that good well; but, an't be satisfated.\n",
            "\n",
            "LUCENTIO:\n",
            "'Tis wisely's a sensell to the king's dogegom and nature:\n",
            "Therefore amonds 't. Come, Katharina! 'Tis he\n",
            "doubtles, are brave fellow. Dare thou me yet?\n",
            "\n",
            "ALORSO:\n",
            "Boldly, good man; for whence art thou do him more run?\n",
            "Now, by my services that we can call'd me, but out retreman\n",
            "Would blue shall poise of her only unpeople, for\n",
            "Mightry, which the recoveth words that lives.\n",
            "\n",
            "JULIET:\n",
            "Gramer aims; thence will out-join'd up his worth.\n",
            "\n",
            "ISABELLA:\n",
            "Would this master Friar Johe fellow?\n",
            "\n",
            "Father:\n",
            "Thou art thieves?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Lovel and in this few, but can be net cast\n",
            "And pay most profits worth the belly.\n",
            "\n",
            "MARCIUS:\n",
            "How now, good for hither?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Comfort! What faults shall I not serve, you hand.\n",
            "\n",
            "BUCKINGHAM:\n",
            "He does, I fear thee, friend, and tarry not; the part heart\n",
            "is 'smemed; then let us a\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}